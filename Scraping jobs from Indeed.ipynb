{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # request accesing pages\n",
    "from bs4 import BeautifulSoup # to scrape html code from pages\n",
    "import re # to cleanup some data points - searches count-\n",
    "from __future__ import division # needed for division\n",
    "import pandas as pd # to create a data frame\n",
    "import numpy as np # generate random times for pauses between requests \n",
    "import time # package for pausing code\n",
    "from time import gmtime, strftime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#importing searches & keywords\n",
    "\n",
    "searches = pd.read_csv('searches.csv')\n",
    "keywords = pd.read_csv('keywords.csv')\n",
    "\n",
    "# Defining a function to transform the desired keywords from a dataframe into a dictionary\n",
    "def dataframeToDict(df):\n",
    "    ncol = df.columns\n",
    "    skilldict = {}\n",
    "    for c in ncol:\n",
    "        cvalues = df[c].values\n",
    "        skilldict[c] = cvalues[~pd.isnull(cvalues)]\n",
    "    return skilldict\n",
    "\n",
    "keywords = dataframeToDict(keywords)\n",
    "seperator = \"_\" \n",
    "\n",
    "start_time = strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Scraping job links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are going to scrape jobs for 1240 searches:\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-9fd40112ffc6>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  searches.job_title[search] = searches.job_title[search].replace('\"', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "\n",
      "There are 11616163200031999150250102100051005150250102100051005715373391837334665934664449422356594948866708888736768237980702141305632358179623583216619132466206894416212944106871549744421795728362446995109680382548482499221030410304311602826050858315293422712077350449113504803466245728662066228341314676581768249114631134369951815651963310220368258666496606668300400047002006002011400441201576623064662665600873736256665956626670013003400330020053002003400060014008001400270002700140040014401426607298166076683164077029554240465365365362955305362404653653602955424047536536536295540536240465365363703699730003676767370040673000367671373299610700437333003333299637815374391121940756219291049122686226850663104935487202281522324107141305632358179623583216619132466206894416212944106871549744421795728362446995109680382548482499221030410304311602826050858315293422712077350449113504803466255728662066228341314676581768249114631134369951815651973311220368258666496606668300400047002006002011400441201576623064662665600873736256665956626670013003400330020053002003400060014008001400270002700140040014401426607298166076683264078029554240465365365362955305362404653653602955424047536536536295540536240465365361380369973000367676737004067300036767037004299966767673699706729996676788871537339183733466593466444942235659494886670888873676823798070214130563235817962358321661913246620689441621294410687154974442179572836244699510968038254848249922103041030431160282605085831529342271207735044911350480346624572866206622834131467658176824911463113436995181565196331022036825866649660666830040004700200600201140044120157662306466266560087373625666595662667001300340033002005300200340006001400800140027000270014004001440142660729816607668316407702955424046536536536295530536240465365360295542404753653653629554053624046536536370369973000367676737004067300036767137329961070043733300333329963711715373391837334665934664449422356594948866708888736768237980702141305632358179623583216619132466206894416212944106871549744421795728362446995109680382548482499221030410304311602826050858315293422712077350449113504803466245728662066228341314676581768249114631134369951815651963310220368258666496606668300400047002006002011400441201576623064662665600873736256665956626670013003400330020053002003400060014008001400270002700140040014401426607298166076683164077029554240465365365362955305362404653653602955424047536536536295540536240465365363703699730003676767370040673000367671373299610700437333003333299637117153733918373346659346644494223565949488667088887367682379807021413056323581796235832166191324662068944162129441068715497444217957283624469951096803825484824992210304103043116028260508583152934227120773504491135048034662457286620662283413146765817682491146311343699518156519633102203682586664966066683004000470020060020114004412015766230646626656008737362566659566266700130034003300200530020034000600140080014002700027001400400144014266072981660766831640770295542404653653653629553053624046536536029554240475365365362955405362404653653637036997300036767673700406730003676713732996107004373330033332996371111111616166640878285334087828533145345313473479794453347347453794934713471061069100644744710610610069 jobs for your [  1 / 1240  ] search!, but we can only scrape 1000.\n",
      "\n",
      "link of this search: https://www.indeed.com/jobs?q=actuarial+analyst&l=san+francisco&radius=5&jt=all&filter=0&limit=50\n",
      "\n",
      "Initializing Scraping 1000 Actuarial Analyst jobs in San Francisco!  [  1 / 1240  ]\n",
      " \n",
      "Scraped 50 jobs [ 5% ]  [  1 / 1240  ]\n",
      "Scraped 100 jobs [ 10% ]  [  1 / 1240  ]\n",
      "Scraped 150 jobs [ 15% ]  [  1 / 1240  ]\n",
      "Scraped 200 jobs [ 20% ]  [  1 / 1240  ]\n",
      "Scraped 250 jobs [ 25% ]  [  1 / 1240  ]\n",
      "Scraped 300 jobs [ 30% ]  [  1 / 1240  ]\n",
      "Scraped 350 jobs [ 35% ]  [  1 / 1240  ]\n",
      "Scraped 400 jobs [ 40% ]  [  1 / 1240  ]\n",
      "Scraped 450 jobs [ 45% ]  [  1 / 1240  ]\n",
      "Scraped 500 jobs [ 50% ]  [  1 / 1240  ]\n",
      "Scraped 550 jobs [ 55% ]  [  1 / 1240  ]\n",
      "Scraped 600 jobs [ 60% ]  [  1 / 1240  ]\n",
      "Scraped 650 jobs [ 65% ]  [  1 / 1240  ]\n",
      "Scraped 700 jobs [ 70% ]  [  1 / 1240  ]\n",
      "Scraped 750 jobs [ 75% ]  [  1 / 1240  ]\n",
      "Scraped 800 jobs [ 80% ]  [  1 / 1240  ]\n",
      "Scraped 850 jobs [ 85% ]  [  1 / 1240  ]\n",
      "Scraped 900 jobs [ 90% ]  [  1 / 1240  ]\n",
      "Scraped 950 jobs [ 95% ]  [  1 / 1240  ]\n",
      "Pausing for 1 seconds\n",
      "-------------------------\n",
      " \n",
      "Finished Scraping 6 jobs [ 0% ]  [  1 / 1240  ]  [  Total: 6 jobs  ]\n",
      " \n",
      "Dont worry if the % or number of jobs dont match, Indeed changes the search results on the go as you browse!\n",
      " \n",
      "-------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-9fd40112ffc6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"-------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mresults_count\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mresults_count\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"NO\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"There are \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults_count\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m  \u001b[1;34m\" jobs for your [  \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" / \"\u001b[0m \u001b[1;33m+\u001b[0m  \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"  ] search!, but we can only scrape 1000.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "p1_start = time.time()\n",
    "\n",
    "# Creating the Data Frame that will hold the scraped data\n",
    "df_final = pd.DataFrame(columns=[\"Search\", \"Title\",\"Location\",\"Company\",\"Salary\", \"Summary\", \"Post_Date\", \"Link\"])\n",
    "print (\"We are going to scrape jobs for \" + str(searches.shape[0]) + \" searches:\")\n",
    "print (\" \")\n",
    "failed = 0\n",
    "for search in range(0,searches.shape[0], 1):\n",
    "    df = pd.DataFrame(columns=[\"Search\", \"Title\",\"Location\",\"Company\",\"Salary\", \"Summary\", \"Post_Date\", \"Link\"])\n",
    "    url = \"https://www.indeed.com/jobs?q=\" + searches.job_title[search].lower().replace(' ', '+') + \\\n",
    "    \"&l=\" + searches.job_location[search].lower().replace(' ', '+') + \"&\" + 'radius=' + str(searches.search_radius[search]) + \"&jt=\" + searches.job_type[search] + \\\n",
    "    \"&filter=0&limit=50\"\n",
    "    # making sure job title does not have \" \"\n",
    "    searches.job_title[search] = searches.job_title[search].replace('\"', '')\n",
    "    \n",
    "    # request and scrape first page results\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.content, 'html.parser', from_encoding=\"utf-8\")\n",
    "    \n",
    "    # scraping the total number of jobs for this search\n",
    "    try:\n",
    "        results_count = soup.select('#searchCount')\n",
    "        # cleaning up the scraped string\n",
    "        results_count = re.sub('\\D', '', str(results_count))[1::]\n",
    "        results_count = int(results_count)\n",
    "    except:\n",
    "        results_count = 'NO'\n",
    "        failed = failed + 1\n",
    "\n",
    "    \n",
    "    print (\"-------------------------\")\n",
    "    print (\"\")\n",
    "    if results_count > 1000 and results_count != \"NO\":\n",
    "        print (\"There are \" + str(results_count) +  \" jobs for your [  \" + str(search + 1) + \" / \" +  str(str(searches.shape[0])) + \"  ] search!, but we can only scrape 1000.\")\n",
    "    else:\n",
    "        print (\"There are \" + str(results_count) +  \" jobs for your [  \" + str(search + 1) + \" / \" +  str(str(searches.shape[0])) + \"  ] search!\")\n",
    "    \n",
    "    print (\"\")\n",
    "    print (\"link of this search: \" + url)\n",
    "    print (\"\")\n",
    "    \n",
    "\n",
    "    # to make sure we only scrape the exact number of job posts, otherwise we will have many duplicates\n",
    "\n",
    "    if results_count < 1000:\n",
    "        target = results_count\n",
    "    else:\n",
    "        target = 1000\n",
    "\n",
    "    # Loop to request different results pages\n",
    "    if results_count != 'NO':\n",
    "        for page in range(0, target, 50):\n",
    "            if page != 0:\n",
    "                progress =  int(page/target * 100)\n",
    "                print (\"Scraped \" + str(page) + \" jobs [ \" + str(progress) + \"% ]  [  \" + str(search + 1) + \" / \" +  str(str(searches.shape[0])) + \"  ]\")\n",
    "\n",
    "            else:\n",
    "\n",
    "                print (\"Initializing Scraping \" + str(target) + \" \" + str(searches.job_title[search]) +\" jobs in \" + searches.job_location[search] + \"!\" + \\\n",
    "                \"  [  \" + str(search + 1) + \" / \" +  str(str(searches.shape[0])) + \"  ]\")\n",
    "                print (\" \")\n",
    "\n",
    "\n",
    "        new_url = url + \"&start=\" + str(page)\n",
    "        html = requests.get(new_url)\n",
    "        soup = BeautifulSoup(html.content, 'html.parser', from_encoding=\"utf-8\")\n",
    "\n",
    "        # Loop to scrape job title, location, company, salary, synopsis, post date, link\n",
    "\n",
    "        for each in soup.find_all(class_= \"result\"):\n",
    "                sponsor = each.find(class_='sponsoredGray') \n",
    "\n",
    "                if sponsor is None: #ignore any post that is not organic (sponsored)\n",
    "\n",
    "                    try:\n",
    "                        date = each.find('span', {'class':'date' }).text.replace('\\n', '')\n",
    "                    except:\n",
    "                        date = 'None'\n",
    "                    try:\n",
    "                        joblink = each.find(class_= 'turnstileLink').get('href')\n",
    "                        link = 'https://www.indeed.com' + joblink\n",
    "                    except:\n",
    "                        link = 'None'\n",
    "                    try: \n",
    "                        title = each.find(class_='jobtitle').text.replace('\\n', '')\n",
    "                    except:\n",
    "                        title = 'None'\n",
    "                    try:\n",
    "                        location = each.find('span', {'class':'location' }).text.replace('\\n', '')\n",
    "                    except:\n",
    "                        location = 'None'\n",
    "                    try: \n",
    "                        company = each.find(class_='company').text.replace('\\n', '')\n",
    "                    except:\n",
    "                        company = 'None'\n",
    "                    try:\n",
    "                        salary = each.find('span', {'class':'no-wrap'}).text.replace('\\n', '')\n",
    "                    except:\n",
    "                        salary = 'None'\n",
    "                    try:\n",
    "                        summary = each.find('span', {'class':'summary'}).text.replace('\\n', '')\n",
    "                    except:\n",
    "                        summary = 'None'\n",
    "        # Append the scraped datapoints into a record in the Data Frame           \n",
    "\n",
    "                    df = df.append({'Title':title, 'Location':location, 'Company':company,\n",
    "                                    'Salary':salary, 'Summary':summary, 'Post_Date':date, 'Link':link,\n",
    "                                    'Search':searches.job_title[search]}, ignore_index=True)\n",
    "\n",
    "        # Pausing the loop for a random value in a normal distribution with a mean of 1 seconds, imitate human browsing     \n",
    "\n",
    "        pausetime = int(np.abs(np.random.randn(1) + 1))  \n",
    "        if page != 0:\n",
    "            print (\"Pausing for \" + str(pausetime) + \" seconds\")\n",
    "            print (\"-------------------------\")\n",
    "            time.sleep(pausetime)\n",
    "    else:\n",
    "        print (\"Skipping  [  \" + str(search + 1) + \" / \" +  str(str(searches.shape[0])) + \"  ] search!\")\n",
    "    df_final = pd.concat([df_final,df])\n",
    "    progress =  int(df.shape[0]/target * 100)  \n",
    "\n",
    "    if results_count != \"NO\":\n",
    "        print (\" \")\n",
    "        print (\"Finished Scraping \" + str(df.shape[0]) + \" jobs [ \" + str(progress) + \"% ]  [  \" + str(search + 1) + \" / \" +  str(searches.shape[0]) + \"  ]  [  Total: \" + str(df_final.shape[0])  + \" jobs  ]\") \n",
    "        print (\" \")\n",
    "    \n",
    "        if df.shape[0] != target:\n",
    "            print (\"Dont worry if the % or number of jobs dont match, Indeed changes the search results on the go as you browse!\")\n",
    "    \n",
    "    print (\" \")\n",
    "print (\" \")\n",
    "print (\" \")\n",
    "print (\"-------------------------\")\n",
    "print (\"Done with the first phase\")\n",
    "scraped = df_final.shape[0]\n",
    "if failed != 0:\n",
    "    print (\"There were \" + str(failed) + \" searches that had no results, so we only scraped jobs for \" + str(searches.shape[0] - failed) + \" out of the \" +  str(searches.shape[0]) + \" searches.\") \n",
    "print (\"Total jobs scraped: \" + str(scraped) + \" jobs.\")\n",
    "\n",
    "df_final.drop_duplicates(keep='first', subset=\"Link\", inplace=True)\n",
    "\n",
    "duplicates  = scraped - df_final.shape[0]\n",
    "\n",
    "print (\"There were \" + str(duplicates) + \" duplicated jobs that we removed.\") \n",
    "print (\"So we only saved \" + str(df_final.shape[0]) + \" jobs.\")\n",
    "df_final.reset_index(drop=True, inplace=True)\n",
    "df1 = df_final\n",
    "\n",
    "p1_duration = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - p1_start))\n",
    "print (\"Duration: \" + p1_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Scraping the body of Job Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final = df1\n",
    "\n",
    "p2_start = time.time()\n",
    "\n",
    "# Loop to request job links collected from the previous phase\n",
    "target_body = df_final.shape[0]\n",
    "df_body = pd.DataFrame(columns=[\"Body\"])\n",
    "\n",
    "for job_link in range(0, target_body, 1):\n",
    "    job_url = df_final.Link.iloc[job_link]\n",
    "\n",
    "    if job_link != 0:\n",
    "        job_progress =  int(job_link/target_body * 100)\n",
    "        print (\"Scraped the Body of \" + str(job_link) + \" jobs [ \" + str(job_progress) + \"% ]  [ \" + str(job_link) + \" / \" +  str(target_body) + \" ]\")\n",
    "\n",
    "    else:\n",
    "        print (\"Initializing Scraping the Body of \" + str(target_body) + \" jobs!\")\n",
    "        print (\" \")\n",
    "    html = requests.get(job_url)\n",
    "    soup = BeautifulSoup(html.content, 'html.parser', from_encoding=\"utf-8\")\n",
    "\n",
    "    # Scrape the body of job link\n",
    "\n",
    "    try:\n",
    "        body = soup.select('.snip')\n",
    "        idx = body[0].getText().find('save job')\n",
    "        body = body[0].getText()[0:idx]\n",
    "    except:\n",
    "        \n",
    "        body = \"None\"\n",
    "\n",
    "    # Append the scraped body in a Data Frame           \n",
    "\n",
    "    df_body = df_body.append({'Body': body}, ignore_index=True)\n",
    "\n",
    "    # Pausing the loop for a random value in a normal distribution with a mean of 1 seconds, imitate human browsing     \n",
    "\n",
    "    pausetime = int(np.abs(np.random.randn(1) + 0))  \n",
    "    if job_link != 0:\n",
    "        print (\"Pausing for \" + str(pausetime) + \" seconds\")\n",
    "\n",
    "        print (\"-------------------------\")\n",
    "        time.sleep(pausetime)\n",
    "\n",
    "progress =  int((job_link+1)/target_body * 100)\n",
    "\n",
    "print (\" \")\n",
    "print (\"Finished Scraping \" + str(df_final.shape[0]) + \" jobs [ \" + str(progress) + \"% ]\")\n",
    "print (\" \")\n",
    "print (\"-------------------------\")\n",
    "print (\"-------------------------\")\n",
    "print (\" \")\n",
    "print (\" \")\n",
    "# Merging the old and new Data Frames\n",
    "\n",
    "df_final = pd.merge(df_final, df_body, left_index=True, right_index=True)\n",
    "\n",
    "scraped = df_final.shape[0]\n",
    "\n",
    "print (\"Total jobs scraped: \" + str(scraped) + \" jobs.\")\n",
    "\n",
    "\n",
    "df_final.drop_duplicates(keep='first', subset=\"Body\", inplace=True)\n",
    "\n",
    "duplicates  = scraped - df_final.shape[0]\n",
    "\n",
    "print (\"We found \" + str(duplicates) + \" duplicated jobs that we removed.\") \n",
    "print (\"So we only saved \" + str(df_final.shape[0]) + \" jobs.\")\n",
    "\n",
    "df_final.reset_index(drop=True, inplace=True)\n",
    "\n",
    "p2_duration = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - p2_start))\n",
    "\n",
    "df2 = df_final\n",
    "print (\"Duration: \" + p2_duration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Text mining the body of Job Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final = df2\n",
    "\n",
    "p3_start = time.time()\n",
    "\n",
    "print (\"Initializing extracting keywords from Body of \" + str(df_final.shape[0]) + \" jobs!\")\n",
    "print (\" \")\n",
    "print (\"-------------------------\")\n",
    "columns = []\n",
    "dictionary_count = len(keywords.keys())\n",
    "#creating columns for keywords\n",
    "for x in range(0, dictionary_count, 1):\n",
    "    \n",
    "    for y in range(1, len(keywords.get(list(keywords)[x])), 1):\n",
    "        keyword = keywords.get(list(keywords)[x])[0] + seperator + keywords.get(list(keywords)[x])[y]\n",
    "        columns.append(keyword)\n",
    "        \n",
    "df_exp = pd.DataFrame(columns= ['Experience', 'Exp Min', 'Exp Max'])\n",
    "\n",
    "df_text = pd.DataFrame(columns= [columns] , index=df_final.index)\n",
    "\n",
    "keyword_count = len(list(df_text.columns.values))\n",
    "\n",
    "#Experience\n",
    "for row in range(0, df_final.shape[0], 1):\n",
    "    try:\n",
    "        idx = df_final.Body[row].find('years')\n",
    "        experience = df_final.Body[row][idx-10:idx+5]\n",
    "        exp = re.sub('\\D', '', experience)\n",
    "        experience = df_final.Body[row][idx-35:idx+35]\n",
    "        if len(exp) > 4 or exp == \"\":\n",
    "            experience = None\n",
    "            expmin = None\n",
    "            expmax = None\n",
    "        else:\n",
    "            if len(exp) == 4:\n",
    "                expmin = exp[0:2]\n",
    "                expmax = exp[2:4]\n",
    "            if len(exp) == 3:\n",
    "                expmin = exp[0]\n",
    "                expmax = exp[1:3]\n",
    "            if len(exp) == 2:\n",
    "                if exp[1] > exp[0]:\n",
    "                    expmin = exp[0]\n",
    "                    expmax = exp[1]\n",
    "                else:\n",
    "                    expmin = exp\n",
    "                    expmax = None\n",
    "                \n",
    "            if len(exp) == 1:\n",
    "                expmin = exp\n",
    "                expmax = None\n",
    "            \n",
    "            if int(expmin) > 20:\n",
    "                expmin = None\n",
    "                expmax = None\n",
    "                experience = None\n",
    "            if expmax != None and (int(expmax) > 20 or int(expmax) <= int(expmin)):\n",
    "                expmin = None\n",
    "                expmax = None\n",
    "                experience = None\n",
    "    except:\n",
    "        experience = None\n",
    "        expmin = None\n",
    "        expmax = None\n",
    "    \n",
    "    df_exp = df_exp.append({'Experience': experience, 'Exp Min': expmin, 'Exp Max' : expmax}, ignore_index=True)\n",
    "    \n",
    "#Keywords    \n",
    "    for key in range(0, keyword_count, 1):\n",
    "            word = list(df_text.columns.values)[key].split(\"_\")[1]\n",
    "            idx = df_final.Body[row].find(word)\n",
    "            \n",
    "            if word in df_final.Body.iloc[row] or (len(word) != 2 and word.lower() in df_final.Body.iloc[row]) :\n",
    "                if df_final.Body.iloc[row][idx-1] != \"H\":\n",
    "                    df_text.iloc[row,key] = 1            \n",
    "            else:\n",
    "                df_text.iloc[row,key] = 0 \n",
    "                \n",
    "    progress = int(row / (df_final.shape[0] - 1) * 100)\n",
    "    if (row+1) % 250 == 0:\n",
    "        print (\"Extracted keywords from the Body of \" + str(row + 1) + \" jobs [ \" + str(progress) + \"% ]\") \n",
    "        print (\"-------------------------\")\n",
    "\n",
    "print (\" \")\n",
    "print (\" \")\n",
    "print (\"Finished extracting keywords from Body of \" + str(df_final.shape[0]) + \" jobs [ \" + str(progress) + \"% ]\")\n",
    "\n",
    "df_final = pd.merge(df_final, df_exp, left_index=True, right_index=True)\n",
    "df_final = pd.merge(df_final, df_text, left_index=True, right_index=True)\n",
    "\n",
    "p3_duration = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - p3_start))\n",
    "\n",
    "df3 = df_final\n",
    "print (\"Duration: \" + p3_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4: Classifying jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final = df3 \n",
    "\n",
    "p4_start = time.time()\n",
    "\n",
    "df_title = pd.DataFrame(columns= [\"General_Title\", \"Department\", \"City\"])\n",
    "df_final = df3\n",
    "for job in  range(0, df_final.shape[0], 1):\n",
    "    point = df_final.Title.iloc[job]\n",
    "    GT = ''\n",
    "    pre = ''\n",
    "    data = ''\n",
    "    post = 'Analyst'\n",
    "    dep = 'General'\n",
    "    if 'Senior ' in point or 'Sr' in point:\n",
    "        pre = 'Senior '\n",
    "#Data\n",
    "    if 'Data' in point:\n",
    "        data = 'Data '\n",
    "        dep = 'Statistics and Data Science'\n",
    "#Scientist\n",
    "    if 'Scientist' in point or 'Science' in point :\n",
    "        post = 'Scientist'\n",
    "        dep = 'Statistics and Data Science'\n",
    "        data = 'Data '\n",
    "#Business\n",
    "    if 'Business' in point or 'BI' in point or 'Intelligence' in point:\n",
    "        GT = 'Business '\n",
    "        dep = 'Business'\n",
    "        \n",
    "#IT\n",
    "    if 'IT' in point or 'System' in point or 'Security' in point or 'Engineer' in point or \\\n",
    "    'Technical' in point or 'Architect' in point or 'SOX' in point or 'Technology' in point:        \n",
    "        GT = 'IT '  \n",
    "        dep = 'IT'\n",
    "        \n",
    "#Marketing\n",
    "    if 'Marketing' in point or 'SEO' in point or 'SEM' in point or 'Campaign' in point or \\\n",
    "    'Product' in point or 'Digital' in point or 'Media' in point or 'Growth' in point or 'Engagement' in point:            \n",
    "        GT = 'Marketing '\n",
    "        dep = 'Marketing'\n",
    "        \n",
    "#Supply Chain    \n",
    "    if 'Supply Chain' in point or 'Logistics' in point or 'Operations' in point or 'Procurement' in point:        \n",
    "        GT = 'Supply Chain ' \n",
    "        dep = 'Supply Chain'\n",
    "        \n",
    "#Finance\n",
    "    if 'Finance' in point or 'Financial' in point or 'Asset' in point or 'Accounting' in point or \\\n",
    "    'Equity' in point or 'Investment' in point or 'Portfolio' in point or 'Banking' in point or \\\n",
    "    'Credit' in point or 'Risk' in point or 'Venture' in point or 'VC' in point or \\\n",
    "    'Securities' in point or 'Fund' in point or 'Investor' in point or 'Venture' in point or \\\n",
    "    'Capital' in point or 'Revenue' in point or 'Loan' in point or 'Wealth' in point or 'FinTech' in point or \\\n",
    "    'Tax' in point:\n",
    "        GT = 'Financial '\n",
    "        dep = 'Finance'\n",
    "#Sales\n",
    "    if 'Sales' in point or 'Operations' in point or 'Account' in point or 'Channel' in point or \\\n",
    "    'Partner' in point or 'Customer' in point or 'Relationship' in point or 'CRM'in point:\n",
    "        GT = 'Sales '\n",
    "        dep = 'Sales'\n",
    "#HR       \n",
    "    if 'HR' in point or 'Human Resources' in point or 'People' in point or 'Staff' in point or \\\n",
    "    'Organizational' in point or 'OD' in point or 'Talent' in point or 'Compensation' in point or \\\n",
    "    'Rewards' in point or 'Payroll' in point or 'Recruiting' in point or 'Benefit' in point:\n",
    "        GT = 'HR '\n",
    "        dep = 'HR'\n",
    "           \n",
    "#City\n",
    "    idx = df_final.Location.iloc[job].find(',')\n",
    "    city = df_final.Location.iloc[job][:idx]    \n",
    "    \n",
    "#Progress        \n",
    "    progress = int(job / (df_final.shape[0] - 1) * 100)\n",
    "\n",
    "    if job % 250 == 0:\n",
    "        print (\"Filtered \" + str(job) + \" jobs [ \" + str(progress) + \"% ]\") \n",
    "        print (\"-------------------------\")        \n",
    "\n",
    "    df_title = df_title.append({'General_Title':(pre + GT + data + post), 'Department': dep , 'City' : city}, ignore_index=True)\n",
    "\n",
    "df_final = pd.merge(df_title, df_final, left_index=True, right_index=True)\n",
    "\n",
    "print (\" \")\n",
    "print (\" \")\n",
    "print (\"Finished filtering \" + str(job) + \" jobs [ \" + str(progress) + \"% ]\")\n",
    "print (\"-------------------------\") \n",
    "\n",
    "p4_duration = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - p4_start))\n",
    "\n",
    "end_time = strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "total_time = time.strftime(\"%H:%M:%S\", time.gmtime(end - start))\n",
    "\n",
    "df4 = df_final\n",
    "print ('We started at ' + start_time)\n",
    "print ('Phase 1 took ' + p1_duration)\n",
    "print ('Phase 2 took ' + p2_duration)\n",
    "print ('Phase 3 took ' + p3_duration)\n",
    "print ('Phase 4 took ' + p4_duration)\n",
    "print ('We finished at ' + end_time)\n",
    "print ('The code ran for ' + total_time)\n",
    "\n",
    "# export dataset\n",
    "file_date = strftime(\"%m-%d-%Y\", gmtime())\n",
    "df_final.to_excel('dataset-' + file_date +'.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Search</th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Post_Date</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Actuarial Analyst</td>\n",
       "      <td>Actuarial Analyst</td>\n",
       "      <td>San Francisco, CA 94111 (Financial District area)</td>\n",
       "      <td>Woodruff Sawyer</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>30+ days ago</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=48b9d04844b16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actuarial Analyst</td>\n",
       "      <td>Actuarial Analyst</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>Woodruff Sawyer</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>30+ days ago</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=ada01ac27bb96...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Actuarial Analyst</td>\n",
       "      <td>Provider Contract/Cost of Care Analyst - Entry...</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>Anthem</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2 days ago</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=9f5203b746670...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Actuarial Analyst</td>\n",
       "      <td>Entry Level Provider Contract/Cost of Care Ana...</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>Anthem</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>14 days ago</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=ea77cff4facaf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Actuarial Analyst</td>\n",
       "      <td>Aon Campus: Actuarial Analyst - West Region</td>\n",
       "      <td>San Francisco, CA 94105 (South Beach area)</td>\n",
       "      <td>Aon Corporation</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>30+ days ago</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=6b34898be9cfb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Actuarial Analyst</td>\n",
       "      <td>Actuarial Analyst/Sr. Actuarial Analyst - Pricing</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>At-Bay</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>30+ days ago</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=237eb6c5428a8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Search                                              Title  \\\n",
       "0  Actuarial Analyst                                  Actuarial Analyst   \n",
       "1  Actuarial Analyst                                  Actuarial Analyst   \n",
       "2  Actuarial Analyst  Provider Contract/Cost of Care Analyst - Entry...   \n",
       "3  Actuarial Analyst  Entry Level Provider Contract/Cost of Care Ana...   \n",
       "4  Actuarial Analyst        Aon Campus: Actuarial Analyst - West Region   \n",
       "5  Actuarial Analyst  Actuarial Analyst/Sr. Actuarial Analyst - Pricing   \n",
       "\n",
       "                                            Location          Company Salary  \\\n",
       "0  San Francisco, CA 94111 (Financial District area)  Woodruff Sawyer   None   \n",
       "1                                  San Francisco, CA  Woodruff Sawyer   None   \n",
       "2                                  San Francisco, CA           Anthem   None   \n",
       "3                                  San Francisco, CA           Anthem   None   \n",
       "4         San Francisco, CA 94105 (South Beach area)  Aon Corporation   None   \n",
       "5                                  San Francisco, CA           At-Bay   None   \n",
       "\n",
       "  Summary     Post_Date                                               Link  \n",
       "0    None  30+ days ago  https://www.indeed.com/rc/clk?jk=48b9d04844b16...  \n",
       "1    None  30+ days ago  https://www.indeed.com/rc/clk?jk=ada01ac27bb96...  \n",
       "2    None    2 days ago  https://www.indeed.com/rc/clk?jk=9f5203b746670...  \n",
       "3    None   14 days ago  https://www.indeed.com/rc/clk?jk=ea77cff4facaf...  \n",
       "4    None  30+ days ago  https://www.indeed.com/rc/clk?jk=6b34898be9cfb...  \n",
       "5    None  30+ days ago  https://www.indeed.com/rc/clk?jk=237eb6c5428a8...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
